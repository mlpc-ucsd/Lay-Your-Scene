<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="description" content="We propose Lay-Your-Scene, a novel text-to-layout generation pipeline for natural scenes using lightweight open-source language models and a diffusion Transformer architecture."><meta name="keywords" content="Lay-Your-Scene, LayouSyn, Text-to-Layout Generation, Diffusion Models, Scene Understanding, Scene Generation, Diffusion Transformers"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers</title><link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.4/css/bulma.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"><link rel="stylesheet" href="./static/css/index.7ce1305d692aa69d31d3.css"><link rel="icon" href="./static/images/mlpc.webp"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script><script src="./static/js/index.21b59bd40255d961c5a1.js"></script></head><body><script>// Immediate scroll reset to prevent any auto-scrolling
    window.scrollTo(0, 0);
    document.documentElement.scrollTop = 0;
    document.body.scrollTop = 0;</script><section class="hero"><div class="hero-body hero-body-less-padding"><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width has-text-centered"><h1 class="title is-1 publication-title">ðŸŽ¨ Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers</h1><div class="column is-full-width is-size-4"><span class="publication-venue"><b>ICCV 2025</b></span></div><div class="is-size-5 publication-authors"><span class="author-block"><a href="https://divyanshsrivastava.com" target="_blank">Divyansh Srivastava</a>,</span> <span class="author-block"><a href="https://xzhang.dev/" target="_blank">Xiang Zhang</a>,</span> <span class="author-block"><a href="" target="_blank">He Wen</a><sup>&dagger;</sup>, </span><span class="author-block"><a href="" target="_blank">Chenru Wen</a><sup>&dagger;</sup>, </span><span class="author-block"><a href="https://pages.ucsd.edu/~ztu/" target="_blank">Zhuowen Tu</a></span></div><div class="is-size-6 publication-authors"><span class="author-block" style="font-size: 90%;"><sup>&dagger;</sup> <i>Project done while interning at UC San Diego.</i></span></div><div class="column has-text-centered"><div class="publication-links"><span class="link-block"><a href="https://arxiv.org/abs/2505.04718v1" class="external-link button is-normal is-rounded is-dark" target="_blank"><span class="icon"><i class="ai ai-arxiv"></i> </span><span>arXiv</span> </a></span><span class="link-block"><a href="https://github.com/mlpc-ucsd/Lay-Your-Scene" class="external-link button is-normal is-rounded is-dark" target="_blank"><span class="icon"><i class="fab fa-github" aria-hidden="true"></i> </span><span>Code</span></a></span></div></div></div></div></div></div></section><section class="teaser"><div class="container is-max-desktop"><div class="hero-body"><div class="columns is-centered has-text-centered"><div class="column is-full-width"><div class="has-text-centered"><img src="./static/images/teaser.webp" alt="Lay-Your-Scene Teaser" class="teaser-image" style="border:none; box-shadow:none; outline:none;"><p class="figure-caption full-width-caption"><strong>Figure 1:</strong> <span class="layousyn">Lay-Your-Scene</span> (shorthand <span class="layousyn">LayouSyn</span>) demonstrates superior scene awareness, generating layouts with high geometric plausibility and strictly adhering to numerical and spatial constraints. Object nouns in the prompts are highlighted with corresponding colors in the layout.</p></div></div></div></div></div></section><section class="section section-less-padding"><div class="container is-max-desktop"><div class="columns is-centered has-text-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Abstract</h2><div class="content has-text-justified"><p>We present <span class="layousyn">Lay-Your-Scene</span> (shorthand <span class="layousyn">LayouSyn</span>), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that <span class="layousyn">LayouSyn</span> outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of <span class="layousyn">LayouSyn</span>. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of <span class="layousyn">LayouSyn</span> in image editing applications.</p></div></div></div></div></section><hr class="section-separator"><section class="section section-less-padding" id="method-section"><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Method</h2><div class="has-text-centered"><img src="./static/images/architecture.webp" alt="Lay-Your-Scene Architecture"><p class="figure-caption full-width-caption"><strong>Figure 2:</strong> Overview of inference pipeline for <span class="layousyn">LayouSyn</span></p></div><br><div class="method-overview"><p>We frame the scene layout generation task as a two-stage process:<br><br></p><ol style="margin-left:1.2em;"><li><strong>Description Set Generation:</strong> A lightweight open-source language model extracts relevant object descriptions from the text prompt. For example, if prompt is "Three people walking on the street", the model outputs a JSON with count of each object, i.e, {"person": 3, "street": 1}.</li><li><strong>Conditional Layout Generation:</strong> A trained, aspect-aware, diffusion model generates layouts conditioned on the text prompt and object descriptions directly in bounding-box space.</li></ol><p></p></div></div></div></div></section><hr class="section-separator"><section class="section section-less-padding qualitative-results" id="qualitative-results-section"><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Qualitative Results</h2></div></div><div class="columns is-centered has-text-centered"><div class="column is-full-width"><div class="has-text-centered figure-with-caption-below" id="figure3"><img src="./static/images/comparative.webp" alt="Qualitative comparisons between LayoutGPT+GLIGEN and LayouSyn+GLIGEN" style="height:600px; width:auto; object-fit:contain;"><p class="figure-caption full-width-caption"><strong>Figure 3:</strong> Comparative analysis with LayoutGPT. In the first example, LayoutGPT produces a semantically incorrect layout, with the table and chairs not positioned under the lamp while our method follows the constraints precisely. In the second example, LayoutGPT generates a geometrically incorrect layout for the cat, whereas our method successfully understands the relationships between different objects and produces a correct layout.</p></div></div></div><div class="columns is-centered has-text-centered"><div class="column is-full-width"><div class="has-text-centered figure-with-caption-below" id="figure4"><img src="./static/images/Diversity_of_Generation.webp" alt="Diversity of generated layouts and aspect ratio variations" style="height:600px; width:auto; object-fit:contain;"><p class="figure-caption full-width-caption"><strong>Figure 4:</strong> Diversity of layouts generated by <span class="layousyn">LayouSyn</span> for the same text prompt.</p></div></div></div><div class="columns is-centered has-text-centered"><div class="column is-full-width"><div class="has-text-centered" id="figure5"><img src="./static/images/aspect_ratio.webp" alt="Layout generation with varying aspect ratios"><p class="figure-caption full-width-caption"><strong>Figure 5:</strong> Layout generation with varying aspect ratios. Layouts generated at different aspect ratios for prompt: <em>"A <span style="color:rgb(50,136,189);">man</span> riding a <span style="color:rgb(244,109,67);">horse</span> on the <span style="color:rgb(94,79,162);">street</span>."</em> The model adjusts the position and aspect ratio of the <span style="color:rgb(50,136,189);">man</span> and the <span style="color:rgb(244,109,67);">horse</span> to produce natural-looking layouts.</p></div></div></div></div></section><hr class="section-separator"><section class="section section-less-padding quantitative-results" id="quantitative-results-section"><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Quantitative Results</h2></div></div><div class="columns is-centered"><div class="column is-full-width"><div class="content has-text-justified"><p>We evaluate <span class="layousyn">LayouSyn</span> on two criteria:</p><ol><li id="layout-quality"><strong>Layout Quality:</strong> We draw the generated layout as an image and map each object to a specific color following document layout generation literature, taking into account semantic similarity between objects based on CLIP similarity. We refer to this metric as L-FID (Layout-FID).<div class="flexible-figure-container" style="margin-top: 2rem;"><div class="figure-image-container"><table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto;"><thead><tr><th style="text-align: left; padding: 12px;">Model</th><th style="text-align: center; padding: 12px;">L-FID â†“</th></tr></thead><tbody><tr><td style="text-align: left; padding: 10px;">LayoutGPT (GPT-3.5)</td><td style="text-align: center; padding: 10px;">3.51</td></tr><tr><td style="text-align: left; padding: 10px;">LayoutGPT (GPT-4o-mini)</td><td style="text-align: center; padding: 10px;">6.72</td></tr><tr><td style="text-align: left; padding: 10px;">Llama-3.1-8B (finetuned)</td><td style="text-align: center; padding: 10px;">13.95</td></tr><tr><td style="text-align: left; padding: 10px; font-weight: bold;"><span class="layousyn">LayouSyn</span></td><td style="text-align: center; padding: 10px; font-weight: bold;">3.07 <span style="color: #28a745;">(+12.5%)</span></td></tr><tr><td style="text-align: left; padding: 10px; font-weight: bold;"><span class="layousyn">LayouSyn</span> (GRIT pretraining)</td><td style="text-align: center; padding: 10px; font-weight: bold;">3.31 <span style="color: #28a745;">(+5.6%)</span></td></tr></tbody></table></div><p class="figure-caption full-width-caption"><strong>Table 1:</strong> Layout Quality Evaluation on the COCO-GR Dataset: Our method outperforms existing layout generation methods on the FID (L-FID) score by at least 5.6%</p></div></li><li id="spatial-numerical-reasoning"><strong>Spatial and Numerical prompt-following ability:</strong> We evaluate our method on the NSR-1K benchmark and assess whether the generated layouts follow specified numerical and spatial constraints in the prompt.</li></ol></div></div></div><div class="columns is-centered has-text-centered" style="margin-top: 2rem;"><div class="column is-full-width"><div class="flexible-figure-container"><div class="figure-image-container"><table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto; font-size: 0.9em;"><thead><tr><th rowspan="2" style="text-align: left; padding: 12px; vertical-align: middle; border-right: 2px solid #dbdbdb;"></th><th colspan="4" style="text-align: center; padding: 12px; border-bottom: 2px solid #dbdbdb;">Numerical Reasoning</th><th colspan="2" style="text-align: center; padding: 12px; border-bottom: 2px solid #dbdbdb;">Spatial Reasoning</th></tr><tr><th style="text-align: center; padding: 8px;">Prec. â†‘</th><th style="text-align: center; padding: 8px;">Recall â†‘</th><th style="text-align: center; padding: 8px;">Acc. â†‘</th><th style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;">GLIP â†‘</th><th style="text-align: center; padding: 8px;">Acc. â†‘</th><th style="text-align: center; padding: 8px;">GLIP â†‘</th></tr></thead><tbody><tr style="background-color: #f9f9f9;"><td style="text-align: left; padding: 10px; font-weight: bold; border-right: 2px solid #dbdbdb;">GT layouts</td><td style="text-align: center; padding: 8px;">100.0</td><td style="text-align: center; padding: 8px;">100.0</td><td style="text-align: center; padding: 8px;">100.0</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;">50.08</td><td style="text-align: center; padding: 8px;">100.00</td><td style="text-align: center; padding: 8px;">57.20</td></tr><tr style="background-color: #e8e8e8;"><td colspan="7" style="text-align: left; padding: 8px; font-weight: bold; font-style: italic;">In-context Learning</td></tr><tr><td style="text-align: left; padding: 10px; border-right: 2px solid #dbdbdb;">LayoutGPT (Llama-3.1-8B)</td><td style="text-align: center; padding: 8px;">78.61</td><td style="text-align: center; padding: 8px;">84.01</td><td style="text-align: center; padding: 8px;">71.71</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;">49.48</td><td style="text-align: center; padding: 8px;">75.40</td><td style="text-align: center; padding: 8px;">47.92</td></tr><tr><td style="text-align: left; padding: 10px; border-right: 2px solid #dbdbdb;">LayoutGPT (GPT-3.5)</td><td style="text-align: center; padding: 8px;">76.29</td><td style="text-align: center; padding: 8px;">86.64</td><td style="text-align: center; padding: 8px;">76.72</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;"><u>54.25</u></td><td style="text-align: center; padding: 8px;">87.07</td><td style="text-align: center; padding: 8px;">56.89</td></tr><tr><td style="text-align: left; padding: 10px; border-right: 2px solid #dbdbdb;">LayoutGPT (GPT-4o-mini)</td><td style="text-align: center; padding: 8px;">73.82</td><td style="text-align: center; padding: 8px;">86.84</td><td style="text-align: center; padding: 8px;">77.51</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;"><u>57.96</u></td><td style="text-align: center; padding: 8px;">92.01</td><td style="text-align: center; padding: 8px;"><u>60.49</u></td></tr><tr style="background-color: #e8e8e8;"><td colspan="7" style="text-align: left; padding: 8px; font-weight: bold; font-style: italic;">Zero-shot</td></tr><tr><td style="text-align: left; padding: 10px; border-right: 2px solid #dbdbdb;">LLMGroundedDiffusion (GPT-4o-mini)</td><td style="text-align: center; padding: 8px;">84.36</td><td style="text-align: center; padding: 8px;">95.94</td><td style="text-align: center; padding: 8px;">89.94</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;">38.56</td><td style="text-align: center; padding: 8px;">72.46</td><td style="text-align: center; padding: 8px;">27.09</td></tr><tr><td style="text-align: left; padding: 10px; border-right: 2px solid #dbdbdb;">LLM Blueprint (GPT-4o-mini)</td><td style="text-align: center; padding: 8px; font-weight: bold;">87.21</td><td style="text-align: center; padding: 8px;">67.29</td><td style="text-align: center; padding: 8px;">38.36</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;">42.24</td><td style="text-align: center; padding: 8px;">73.52</td><td style="text-align: center; padding: 8px;">50.21</td></tr><tr style="background-color: #e8e8e8;"><td colspan="7" style="text-align: left; padding: 8px; font-weight: bold; font-style: italic;">Trained / Finetuned</td></tr><tr><td style="text-align: left; padding: 10px; border-right: 2px solid #dbdbdb;">LayoutTransformer *</td><td style="text-align: center; padding: 8px;">75.70</td><td style="text-align: center; padding: 8px;">61.69</td><td style="text-align: center; padding: 8px;">22.26</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;">40.55</td><td style="text-align: center; padding: 8px;">6.36</td><td style="text-align: center; padding: 8px;">28.13</td></tr><tr><td style="text-align: left; padding: 10px; border-right: 2px solid #dbdbdb;">Ranni</td><td style="text-align: center; padding: 8px;">56.23</td><td style="text-align: center; padding: 8px;">83.28</td><td style="text-align: center; padding: 8px;">40.80</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;">38.19</td><td style="text-align: center; padding: 8px;">53.29</td><td style="text-align: center; padding: 8px;">24.38</td></tr><tr><td style="text-align: left; padding: 10px; border-right: 2px solid #dbdbdb;">Llama-3.1-8B (finetuned)</td><td style="text-align: center; padding: 8px;">79.33</td><td style="text-align: center; padding: 8px;">93.36</td><td style="text-align: center; padding: 8px;">70.84</td><td style="text-align: center; padding: 8px; border-right: 2px solid #dbdbdb;">44.72</td><td style="text-align: center; padding: 8px;">86.64</td><td style="text-align: center; padding: 8px;">52.93</td></tr><tr style="background-color: #e8e8e8;"><td colspan="7" style="text-align: left; padding: 8px; font-weight: bold; font-style: italic;">Ours</td></tr><tr><td style="text-align: left; padding: 10px; font-weight: bold; border-right: 2px solid #dbdbdb;"><span class="layousyn">LayouSyn</span></td><td style="text-align: center; padding: 8px;">77.62</td><td style="text-align: center; padding: 8px;">99.23</td><td style="text-align: center; padding: 8px;">95.14</td><td style="text-align: center; padding: 8px;"><u>56.17</u></td><td style="text-align: center; padding: 8px;">87.49</td><td style="text-align: center; padding: 8px;">54.91</td></tr><tr><td style="text-align: left; padding: 10px; font-weight: bold; border-right: 2px solid #dbdbdb;"><span class="layousyn">LayouSyn</span> (GRIT pretraining)</td><td style="text-align: center; padding: 8px;">77.62</td><td style="text-align: center; padding: 8px; font-weight: bold;"><strong>99.23</strong></td><td style="text-align: center; padding: 8px; font-weight: bold;"><strong>95.14</strong></td><td style="text-align: center; padding: 8px;"><u>56.20</u></td><td style="text-align: center; padding: 8px; font-weight: bold;"><strong>92.58</strong></td><td style="text-align: center; padding: 8px;"><u>58.94</u></td></tr></tbody></table></div><p class="figure-caption full-width-caption"><strong>Table 2:</strong> Spatial and numerical reasoning evaluation on the NSR-1K Benchmark. <span class="layousyn">LayouSyn</span> outperforms existing methods on spatial and counting reasoning tasks, achieving state-of-the-art performance on most metrics. Note: * indicates metrics reported by LayoutGPT. We <strong>bold</strong> values for metrics where our method (GT) is 100% and underline values where methods exceed the ground truth performance.</p></div></div></div></div></section><hr class="section-separator"><section class="section section-less-padding applications" id="applications-section"><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width"><h2 class="title is-3 abstract">Applications</h2></div></div><div class="columns is-centered"><div class="column is-full-width"><div class="content has-text-justified"><ul class="results-list" style="list-style: none; margin: 0; padding-left: 0;"></ul><li id="llm-integration"><strong>LLM Integration:</strong> <span class="layousyn">LayouSyn</span> can be integrated with an LLM, using its planned layouts as initialization and refining them to achieve better performance with equal or fewer sampling steps. We demonstrate the improvement on the NSR-1K spatial reasoning benchmark:<div class="flexible-figure-container"><div class="figure-image-container"><table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto;"><thead><tr><th style="text-align: left; padding: 12px;">Method</th><th style="text-align: center; padding: 12px;">Llama-3.1-8B</th><th style="text-align: center; padding: 12px;">GPT-3.5</th><th style="text-align: center; padding: 12px;">GPT-4o-mini</th></tr></thead><tbody><tr><td style="text-align: left; padding: 10px;">Original</td><td style="text-align: center; padding: 10px;">75.40</td><td style="text-align: center; padding: 10px;">87.07</td><td style="text-align: center; padding: 10px;">92.01</td></tr><tr><td style="text-align: left; padding: 10px;">Description Set</td><td style="text-align: center; padding: 10px;">89.75</td><td style="text-align: center; padding: 10px;">90.04</td><td style="text-align: center; padding: 10px;">90.95</td></tr><tr><td style="text-align: left; padding: 10px; font-weight: bold;">Description Set + Inv (15)</td><td style="text-align: center; padding: 10px; font-weight: bold;">90.46</td><td style="text-align: center; padding: 10px; font-weight: bold;">92.37</td><td style="text-align: center; padding: 10px; font-weight: bold;">92.08</td></tr></tbody></table></div><p class="figure-caption full-width-caption"><strong>Table 3:</strong> Spatial reasoning results with LLM initialization. We take the outputs from LayoutGPT with different LLMs (Original) and design two strategies: 1) <em>Description set only</em>: Use only the description sets predicted by the LayoutGPT and perform denoising starting from Gaussian noise with full 100 denoising steps; 2) <em>Description Set + Inversion</em>: in addition to using the description sets, apply DDIM inversion on the bounding boxes predicted by the LLM and denoise for the same number of steps as inversion</p></div></li><li id="image-editing"><strong>Image Editing Pipeline:</strong> <span class="layousyn">LayouSyn</span> enables automated pipeline for adding objects to images. The pipeline: (1) extract relevant objects from the prompt with a lightweight LLM, (2) detect existing objects in the scene with Grounding DINO, (3) complete the layout for the new object with <span class="layousyn">LayouSyn</span>, and (4) inpaint the object into the image with GLIGEN inpainting pipeline.</li></div></div></div><div class="columns is-centered has-text-centered"><div class="column is-full-width"><div class="flexible-figure-container" id="figure10"><div class="figure-image-container"><img src="./static/images/addition_examples.webp" alt="Image editing application"></div><p class="figure-caption full-width-caption"><strong>Figure 6:</strong> Examples of automated object addition using <span class="layousyn">LayouSyn</span>.</p></div></div></div></div></section><hr class="section-separator"><section class="section" id="BibTeX"><div class="container is-max-desktop content"><h2 class="title">BibTeX</h2><pre><code>
    @article{srivastava2025layyourscenenaturalscenelayout,
        title={Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers}, 
        author={Divyansh Srivastava and Xiang Zhang and He Wen and Chenru Wen and Zhuowen Tu},
        year={2025},
        eprint={2505.04718},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2505.04718},
    }
      </code></pre></div></section><footer class="footer"><div class="container"><div class="content has-text-centered"><a class="icon-link" href="https://arxiv.org/abs/2505.04718v1" target="_blank"><i class="fas fa-file-pdf"></i> </a><a class="icon-link" href="https://github.com/mlpc-ucsd/Lay-Your-Scene" target="_blank" class="external-link"><i class="fab fa-github" aria-hidden="true"></i></a></div><div class="columns is-centered"><div class="column is-8"><div class="content"><p>This website is adapted from the amazing template of <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">nerfies</a>.</p></div></div></div></div></footer><button id="go-to-top" class="go-to-top-btn" title="Go to top">â†‘</button><script>// Go to top functionality
    document.addEventListener('DOMContentLoaded', function () {
      // Prevent auto-scroll on page load
      setTimeout(function () {
        window.scrollTo(0, 0);
        document.documentElement.scrollTop = 0;
        document.body.scrollTop = 0;
      }, 100);

      // Also prevent hash navigation
      if (window.location.hash) {
        history.replaceState(null, null, window.location.pathname);
      }

      // Additional scroll prevention on window load
      window.addEventListener('load', function () {
        setTimeout(function () {
          window.scrollTo(0, 0);
        }, 50);
      });

      const goToTopBtn = document.getElementById('go-to-top');

      // Add click event listener
      goToTopBtn.addEventListener('click', function () {
        window.scrollTo({
          top: 0,
          behavior: 'smooth'
        });
      });

      // Show/hide button based on scroll position
      window.addEventListener('scroll', function () {
        if (window.pageYOffset > 100) {
          goToTopBtn.style.visibility = 'visible';
          goToTopBtn.style.opacity = '1';
        } else {
          goToTopBtn.style.opacity = '0';
          setTimeout(function () {
            if (window.pageYOffset <= 100) {
              goToTopBtn.style.visibility = 'hidden';
            }
          }, 300);
        }
      });

      // Button starts hidden
      goToTopBtn.style.visibility = 'hidden';
      goToTopBtn.style.opacity = '0';
    });</script></body></html>